
# åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„FashionMNISTåˆ†ç±»

<div id="progress-container">
  <div id="progress-bar"></div>
</div>

##  å†™åœ¨å‰é¢

ä¸€ç›´æœ‰å†æ¢MLçš„æ¬²æœ›ï¼Œä½†ä¸€ç›´æ²¡å•¥åŠ¨åŠ›ã€‚æ­£å¥½ä»¥æ­¤æ¬¡å¤§ä½œä¸šä¸ºå¥‘æœºï¼ŒåŸºäºé¡¹ç›®ç£ä¿ƒè‡ªå·±å­¦ä¹ 

å…¨æ–‡åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š[CNNä»‹ç»](#what-is-cnn)ä¸[Fashion MNISTåˆ†ç±»å®æˆ˜](#putting-cnn-to-work)

å‰è€…é™¤äº†CNNçš„ä¸€äº›åŸºæœ¬æ¦‚å¿µè¯´æ˜ï¼Œè¿˜å°è¯•é€šè¿‡é€è¡Œä»£ç åˆ†æè§£æ„åŸºæœ¬çš„æœºå™¨å­¦ä¹ æ¡†æ¶

åè€…åˆ™æ˜¯å°†ç†è®ºè¿ç”¨äºå®è·µä¸­ï¼Œä½†è¾ƒä¸ºç²—ç³™


---

## WHAT IS CNNï¼Ÿ

æœ¬èŠ‚å¯¹å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networkï¼ŒCNNï¼‰ï¼Œåœ¨ç®—æ³•ä¸ä»£ç ä¸¤ä¸ªå±‚é¢è¿›è¡Œä»‹ç»ã€‚

### CNN ä»‹ç»

åŒ…æ‹¬ CNN åŸºæœ¬æ¦‚å¿µä¸å·¥ä½œæµç¨‹ã€‚

---


å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤„ç†å…·æœ‰ç½‘æ ¼ç»“æ„æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

CNN çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å·ç§¯æ“ä½œï¼ˆConvolution Operationï¼‰æå–å›¾åƒçš„å±€éƒ¨ç‰¹å¾ï¼Œå¹¶é€šè¿‡å¤šå±‚ç½‘ç»œç»“æ„é€æ­¥ç»„åˆè¿™äº›ç‰¹å¾ï¼Œæœ€ç»ˆå®ç°åˆ†ç±»ã€æ£€æµ‹ç­‰ä»»åŠ¡ã€‚CNN åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»ï¼ˆImage Classificationï¼‰ã€ç›®æ ‡æ£€æµ‹ï¼ˆObject Detectionï¼‰ã€è¯­ä¹‰åˆ†å‰²ï¼ˆSemantic Segmentationï¼‰ç­‰ä»»åŠ¡ã€‚

----


#### CNNçš„ä¸€äº›åŸºæœ¬æ¦‚å¿µ

=== "**å±€éƒ¨æ„Ÿå—é‡ï¼ˆLocal Receptive Fieldï¼‰**"

    CNN é€šè¿‡å·ç§¯æ ¸ï¼ˆFilter/Kernelï¼‰åœ¨è¾“å…¥å›¾åƒä¸Šæ»‘åŠ¨ï¼Œæ¯æ¬¡åªå…³æ³¨å›¾åƒçš„ä¸€ä¸ªå±€éƒ¨åŒºåŸŸï¼ˆç§°ä¸ºæ„Ÿå—é‡ï¼‰ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå›¾åƒã€‚

    è¿™ç§å±€éƒ¨è¿æ¥çš„æ–¹å¼å¤§å¤§å‡å°‘äº†å‚æ•°é‡ï¼ŒåŒæ—¶ä¿ç•™äº†å›¾åƒçš„å±€éƒ¨ç‰¹å¾ã€‚

    æ¯”å¦‚ï¼Œä¸€ä¸ª 3x3 çš„å·ç§¯æ ¸æ¯æ¬¡åªå¤„ç†è¾“å…¥å›¾åƒçš„ 3x3 åŒºåŸŸã€‚

=== "**æƒå€¼å…±äº«ï¼ˆWeight Sharingï¼‰**"

    å·ç§¯æ ¸åœ¨å›¾åƒä¸Šæ»‘åŠ¨æ—¶ï¼Œä½¿ç”¨çš„æ˜¯ç›¸åŒçš„æƒé‡å‚æ•°ã€‚è¿™ç§æƒå€¼å…±äº«æœºåˆ¶è¿›ä¸€æ­¥å‡å°‘äº†æ¨¡å‹çš„å‚æ•°é‡ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹å¯¹å¹³ç§»ä¸å˜æ€§ï¼ˆTranslation Invarianceï¼‰çš„æ•æ‰èƒ½åŠ›ã€‚è¿™ä½¿å¾—æ— è®ºç‰¹å¾å‡ºç°åœ¨å›¾åƒçš„å“ªä¸ªä½ç½®ï¼Œå·ç§¯æ ¸éƒ½èƒ½æ£€æµ‹åˆ°å®ƒã€‚

=== "**å±‚æ¬¡åŒ–ç‰¹å¾æå–ï¼ˆHierarchical Feature Extractionï¼‰**"

    CNN é€šè¿‡å¤šå±‚å·ç§¯å’Œæ± åŒ–æ“ä½œï¼Œé€æ­¥æå–ä»ä½çº§åˆ°é«˜çº§çš„ç‰¹å¾ã€‚ä½çº§ç‰¹å¾ï¼ˆå¦‚è¾¹ç¼˜ã€çº¹ç†ï¼‰åœ¨æµ…å±‚æå–ï¼Œè€Œé«˜çº§ç‰¹å¾ï¼ˆå¦‚ç‰©ä½“å½¢çŠ¶ã€è¯­ä¹‰ä¿¡æ¯ï¼‰åœ¨æ·±å±‚æå–ã€‚

=== "**éçº¿æ€§æ¿€æ´»ï¼ˆNon-linear Activationï¼‰**"

    åœ¨å·ç§¯æ“ä½œåï¼Œé€šå¸¸ä¼šä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰å¼•å…¥éçº¿æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ã€‚å¸¸ç”¨æ¿€æ´»å‡½æ•°æœ‰ReLUï¼ˆRectified Linear Unitï¼‰ã€LeakyReLUã€ELU ç­‰ã€‚

---


#### å·¥ä½œæµç¨‹

æ ¹æ®FashionMNISTæ•°æ®é›†ç‰¹ç‚¹ï¼Œä»‹ç»æœ¬å®éªŒä½¿ç”¨çš„CNNæ¨¡å‹å·¥ä½œæµç¨‹ï¼Œç»“æ„å›¾[^1]è§å›¾1ã€‚

[^1]: ç»“æ„å›¾ç»˜åˆ¶å·¥å…·ï¼šhttp://alexlenail.me/NN-SVG/LeNet.html


![CNN](https://cdn.jsdelivr.net/gh/dixiLOG/blogStatic/20250211cnn.svg)


<center style="font-size:14px;color:#C0C0C0;">å›¾1 ç¥ç»ç½‘ç»œç»“æ„å›¾</center>

1. **è¾“å…¥å›¾åƒï¼ˆInput Imageï¼‰**  

   - è¾“å…¥å›¾åƒçš„å°ºå¯¸ä¸º `1@28x28`ï¼Œè¡¨ç¤ºå•é€šé“ï¼ˆç°åº¦ï¼‰çš„ 28x28 åƒç´ å›¾åƒ


2. **ç¬¬ä¸€å±‚å·ç§¯ï¼ˆConvolution 1ï¼‰**

   - ä½¿ç”¨ 32 ä¸ªå·ç§¯æ ¸ï¼ˆFiltersï¼‰ï¼Œæ¯ä¸ªå·ç§¯æ ¸çš„å°ºå¯¸ä¸º `3x3`ï¼Œæ­¥é•¿ä¸º 1ï¼Œå¡«å……ä¸º 1

   - è¾“å…¥å›¾åƒç»è¿‡å·ç§¯æ“ä½œåï¼Œç”Ÿæˆ 32 ä¸ªç‰¹å¾å›¾ï¼ˆFeature Mapsï¼‰ï¼Œæ¯ä¸ªç‰¹å¾å›¾çš„å°ºå¯¸ä¸º `28x28`

   - **è¾“å‡º**ï¼š`32@28x28`


3. **ç¬¬ä¸€å±‚æœ€å¤§æ± åŒ–ï¼ˆMax-Pool 1ï¼‰**

   - å¯¹å·ç§¯åçš„ç‰¹å¾å›¾è¿›è¡Œæœ€å¤§æ± åŒ–æ“ä½œï¼ˆMax Poolingï¼‰ï¼Œæ± åŒ–çª—å£å°ºå¯¸ä¸º `2x2`ï¼Œæ­¥é•¿ä¸º 2

- æ± åŒ–æ“ä½œå°†ç‰¹å¾å›¾çš„å°ºå¯¸å‡åŠ

  - **è¾“å‡º**ï¼š`32@14x14`

4. **ç¬¬äºŒå±‚å·ç§¯ï¼ˆConvolution 2ï¼‰**

   - ä½¿ç”¨ 64 ä¸ªå·ç§¯æ ¸ï¼Œæ¯ä¸ªå·ç§¯æ ¸çš„å°ºå¯¸ä¸º `3x3`ï¼Œæ­¥é•¿ä¸º 1ï¼Œå¡«å……ä¸º 1

   - è¾“å…¥ä¸º `32@14x14` çš„ç‰¹å¾å›¾ï¼Œç»è¿‡å·ç§¯æ“ä½œåï¼Œç”Ÿæˆ 64 ä¸ªç‰¹å¾å›¾ï¼Œæ¯ä¸ªç‰¹å¾å›¾çš„å°ºå¯¸ä¸º `14x14`

   - **è¾“å‡º**ï¼š`64@14x14`


5. **ç¬¬äºŒå±‚æœ€å¤§æ± åŒ–ï¼ˆMax-Pool 2ï¼‰**

   - å¯¹å·ç§¯åçš„ç‰¹å¾å›¾è¿›è¡Œæœ€å¤§æ± åŒ–æ“ä½œï¼Œæ± åŒ–çª—å£å°ºå¯¸ä¸º `2x2`ï¼Œæ­¥é•¿ä¸º 2

   - æ± åŒ–æ“ä½œå°†ç‰¹å¾å›¾çš„å°ºå¯¸å‡åŠ

   - **è¾“å‡º**ï¼š`64@7x7`


6. **å±•å¹³ï¼ˆFlattenï¼‰**
   - å°†æ± åŒ–åçš„ç‰¹å¾å›¾å±•å¹³ä¸ºä¸€ç»´å‘é‡

   - è¾“å…¥ä¸º `64@7x7`ï¼Œå±•å¹³åçš„å‘é‡é•¿åº¦ä¸º `64 * 7 * 7 = 3136`
   - **è¾“å‡º**ï¼š`1x3136`

7. **ç¬¬ä¸€å±‚å…¨è¿æ¥ï¼ˆFully Connected Layer 1ï¼‰**
   - å°†å±•å¹³åçš„å‘é‡è¾“å…¥å…¨è¿æ¥å±‚ï¼Œå…¨è¿æ¥å±‚çš„è¾“å‡ºå°ºå¯¸ä¸º `1x128`

   - **è¾“å‡º**ï¼š`1x128`

8. **ç¬¬äºŒå±‚å…¨è¿æ¥ï¼ˆFully Connected Layer 2ï¼‰**

   - å°†ç¬¬ä¸€å±‚å…¨è¿æ¥å±‚çš„è¾“å‡ºè¾“å…¥ç¬¬äºŒå±‚å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºå°ºå¯¸ä¸º `1x10`ï¼Œè¡¨ç¤º 10 ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ

   - **è¾“å‡º**ï¼š`1x10`

---

#### è¿˜æœ‰ä»€ä¹ˆï¼Ÿ

> CNNçš„å†å²å‰¯æœ¬æ˜¯ä¸å¯èƒ½é€šè¿‡ä¸€ä¸ªä½œä¸šå…¨éƒ¨æ´æ‚‰çš„ï¼Œæ•…æœ€ç»ˆé€‰æ‹©æŠ›å‡ºä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æ¨¡å‹
>
> åœ¨åç»­çš„æ¨¡å‹å¯¹æ¯”ä¸­ï¼Œä½ è¿˜ä¼šå†çœ‹è§ä»–ä»¬~

=== "ResNet"

    ResNet æ˜¯ä¸€ç§åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥äº† **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰** ï¼Œä½¿å¾—ç½‘ç»œå¯ä»¥æ„å»ºéå¸¸æ·±çš„ç½‘ç»œï¼ˆå¦‚ ResNet-152ï¼‰ï¼Œå¹¶é€šè¿‡æ®‹å·®è¿æ¥æœ‰æ•ˆåœ°è®­ç»ƒè¿™äº›æ·±å±‚ç½‘ç»œï¼ŒåŒæ—¶é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ResNet åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç»å…¸æ¨¡å‹ä¹‹ä¸€ã€‚

    ResNet çš„åŸºæœ¬æ¦‚å¿µåŒ…æ‹¬ **æ®‹å·®å—ï¼ˆResidual Blockï¼‰** å’Œ **å±‚æ¬¡åŒ–ç‰¹å¾æå–** ã€‚æ¯ä¸ªæ®‹å·®å—åŒ…å«ä¸¤ä¸ªå·ç§¯å±‚å’Œä¸€ä¸ªè·³è·ƒè¿æ¥ï¼ˆShortcut Connectionï¼‰ï¼Œè·³è·ƒè¿æ¥å°†è¾“å…¥ç›´æ¥åŠ åˆ°å·ç§¯å±‚çš„è¾“å‡ºä¸Šï¼Œå½¢æˆæ®‹å·®å­¦ä¹ ã€‚é€šè¿‡å¤šä¸ªæ®‹å·®å—çš„å †å ï¼ŒResNet èƒ½å¤Ÿé€æ­¥ä»å›¾åƒä¸­æå–ä»ä½çº§åˆ°é«˜çº§çš„ç‰¹å¾ã€‚

    ResNet çš„å·¥ä½œæµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œé€šè¿‡ **åˆå§‹å·ç§¯å±‚** å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå·ç§¯æ“ä½œï¼Œæå–åˆæ­¥ç‰¹å¾ï¼›æ¥ç€ï¼Œé€šè¿‡ **æ®‹å·®å—å †å ** é€æ­¥æå–æ›´å¤æ‚çš„ç‰¹å¾ï¼›ç„¶åï¼Œä½¿ç”¨ **å…¨å±€å¹³å‡æ± åŒ–** å¯¹ç‰¹å¾å›¾è¿›è¡Œä¸‹é‡‡æ ·ï¼Œé™ä½ç»´åº¦ï¼›æœ€åï¼Œå°†æ± åŒ–åçš„ç‰¹å¾è¾“å…¥ **å…¨è¿æ¥å±‚** ï¼Œå®Œæˆåˆ†ç±»ä»»åŠ¡ã€‚è¿™ç§å±‚æ¬¡åŒ–çš„è®¾è®¡ä½¿å¾— ResNet åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

=== "VIT"

    Transformer å¼•å…¥å›¾åƒå¤„ç†é¢†åŸŸï¼Œçªç ´äº†ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å±€éƒ¨æ„Ÿå—é‡é™åˆ¶ã€‚ViT é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰æ•æ‰å›¾åƒçš„å…¨å±€ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å›¾åƒä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ä¸ä¼ ç»Ÿçš„ CNN ä¸åŒï¼ŒViT å®Œå…¨åŸºäº Transformerï¼Œä¸ä½¿ç”¨å·ç§¯æ“ä½œï¼Œè¿™ä½¿å¾—å®ƒåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶è¡¨ç°å‡ºè‰²ã€‚

    ViT çš„åŸºæœ¬æ¦‚å¿µåŒ…æ‹¬ **å›¾åƒåˆ†å—ï¼ˆPatch Embeddingï¼‰** ã€ **ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰å’Œ Transformer ç¼–ç å™¨** ã€‚é¦–å…ˆï¼Œè¾“å…¥å›¾åƒè¢«åˆ†å‰²æˆå›ºå®šå¤§å°çš„å—ï¼ˆå¦‚ 16x16ï¼‰ï¼Œæ¯ä¸ªå—è¢«å±•å¹³ä¸ºå‘é‡ï¼Œå¹¶é€šè¿‡çº¿æ€§æŠ•å½±æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ã€‚ä¸ºäº†ä¿ç•™å›¾åƒçš„ç©ºé—´ä¿¡æ¯ï¼ŒViT ä¸ºæ¯ä¸ªå›¾åƒå—æ·»åŠ ä½ç½®ç¼–ç ã€‚éšåï¼Œä½¿ç”¨å¤šå±‚ Transformer ç¼–ç å™¨å¯¹å›¾åƒå—è¿›è¡Œç‰¹å¾æå–ï¼Œæ¯å±‚ç¼–ç å™¨åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚

    ViT çš„å·¥ä½œæµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå°†è¾“å…¥å›¾åƒ **åˆ†å‰²æˆå¤šä¸ªå°å—** ï¼Œå¹¶å°†è¿™äº›å—æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼›æ¥ç€ï¼Œä¸ºæ¯ä¸ªå›¾åƒå— **æ·»åŠ ä½ç½®ç¼–ç ** ï¼Œä»¥ä¿ç•™ç©ºé—´ä¿¡æ¯ï¼›ç„¶åï¼Œé€šè¿‡ **å¤šå±‚ Transformer ç¼–ç å™¨** æå–ç‰¹å¾ï¼›æœ€åï¼Œä½¿ç”¨ **åˆ†ç±»æ ‡è®°ï¼ˆCLS Tokenï¼‰** çš„è¾“å‡ºè¿›è¡Œæœ€ç»ˆåˆ†ç±»ã€‚è¿™ç§åŸºäº Transformer çš„è®¾è®¡ä½¿å¾— ViT åœ¨å¤„ç†å¤æ‚å›¾åƒå’Œå…¨å±€ä¿¡æ¯æ—¶è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦åˆ›æ–°ã€‚


!!! note "æ¨¡å‹å¯¹æ¯”"
    ä¸¤ç§æ¨¡å‹ï¼Œå‰è€…æ˜¯ç°ä»£CNNæ¨¡å‹çš„<span style="border-bottom: 1.5px dashed orange;">å®ˆé—¨å‘˜</span>ï¼Œå¦ä¸€è€…åˆ™ä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„<span style="border-bottom: 1.5px dashed orange;">é©æ–°è€…</span>  

    åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå„æœ‰ä¼˜åŠ¿

    ResNet åœ¨<span style="border-bottom: 1.5px dashed orange;">ä¼ ç»Ÿä»»åŠ¡</span>ä¸­è¡¨ç°ä¼˜å¼‚

    è€Œ ViT åœ¨å¤„ç†<span style="border-bottom: 1.5px dashed orange;">å¤æ‚å›¾åƒå’Œå…¨å±€ä¿¡æ¯</span>æ—¶è¡¨ç°å‡ºè‰²


---

### æœ€ç®€å•çš„CNNåˆ†ç±»æ¨¡å‹ï¼šé€è¡Œè§£æ


æ—¨åœ¨å˜æ¸…MLæ¨¡å‹åŸºæœ¬æ¶æ„ã€‚

!!! tip "äººç”Ÿè‹¦çŸ­ï¼Œæˆ‘é€‰python"
    - æœ¬èŠ‚ä»£ç å¤šä¸ºdemoï¼Œå¯èƒ½è·‘ä¸èµ·æ¥  
    - å¸¦é€è¡Œçš„æ³¨é‡Šçš„å®Œæ•´ä»£ç è§[é™„å½•](#_10)


æŒ‰ç…§æƒ¯ä¾‹ï¼Œå…ˆä¸Šå›¾~

``` mermaid
flowchart TD
    A[å¼€å§‹] --> B[ä¾èµ–åº“å¯¼å…¥ä¸åŸºæœ¬é…ç½®]
    subgraph B [ä¾èµ–åº“å¯¼å…¥ä¸åŸºæœ¬é…ç½®]
        direction LR
        B1[å¯¼å…¥å¿…è¦çš„åº“] --> B2[è®¾ç½®éšæœºç§å­] --> B3[é…ç½®è®¾å¤‡ï¼ˆCPU/GPUï¼‰]
    end

    B --> C[æ•°æ®é›†å¯¼å…¥ä¸å¤„ç†]
    subgraph C [æ•°æ®é›†å¯¼å…¥ä¸å¤„ç†]
        direction LR
        C1[å®šä¹‰æ•°æ®é¢„å¤„ç†] --> C2[åŠ è½½æ•°æ®é›†] --> C3[åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†] --> C4[åˆ›å»º DataLoader]
    end

    C --> D[æ¨¡å‹åˆ›å»ºä¸å®ä¾‹åŒ–]
    subgraph D [æ¨¡å‹åˆ›å»ºä¸å®ä¾‹åŒ–]
        direction LR
        D1[å®šä¹‰ CNN æ¨¡å‹ç±»] --> D2[å®ä¾‹åŒ–æ¨¡å‹] --> D3[å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡] --> D4[å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨]
    end

    D --> E[è®­ç»ƒä¸è¯„ä¼°]
    subgraph E [è®­ç»ƒä¸è¯„ä¼°]
        direction LR
        E1[å®šä¹‰è®­ç»ƒå‡½æ•°] --> E2[å®šä¹‰éªŒè¯å‡½æ•°] --> E3[è®­ç»ƒæ¨¡å‹] --> E4[æµ‹è¯•æ¨¡å‹] --> E5[è¾“å‡ºæµ‹è¯•å‡†ç¡®ç‡]
    end

    E --> F[ç»“æŸ]

    %% æ ·å¼ä¼˜åŒ– - ä¸»æ¨¡å—
    style A fill:#8ECFC9,color:white,stroke:#8ECFC1,stroke-width:1px,stroke-dasharray:3
    style B fill:#82B0D2,color:white,stroke:#82B0D1,stroke-width:1px
    style C fill:#FFBE7A,color:white,stroke:#FFBE71,stroke-width:1px
    style D fill:#BEB8DC,color:white,stroke:#BEB8D1,stroke-width:1px
    style E fill:#E7DAD2,color:white,stroke:#E7DAD1,stroke-width:1px
    style F fill:#8ECFC9,color:white,stroke:#8ECFC1,stroke-width:1px,stroke-dasharray:3

    %% æ ·å¼ä¼˜åŒ– - å­æ¨¡å—
    style B1 fill:#BBDEFB,color:#000,stroke:#64B5F6,stroke-width:1px
    style B2 fill:#BBDEFB,color:#000,stroke:#64B5F6,stroke-width:1px
    style B3 fill:#BBDEFB,color:#000,stroke:#64B5F6,stroke-width:1px

    style C1 fill:#FFE0B2,color:#000,stroke:#FFB74D,stroke-width:1px
    style C2 fill:#FFE0B2,color:#000,stroke:#FFB74D,stroke-width:1px
    style C3 fill:#FFE0B2,color:#000,stroke:#FFB74D,stroke-width:1px
    style C4 fill:#FFE0B2,color:#000,stroke:#FFB74D,stroke-width:1px

    style D1 fill:#E1BEE7,color:#000,stroke:#BA68C8,stroke-width:1px
    style D2 fill:#E1BEE7,color:#000,stroke:#BA68C8,stroke-width:1px
    style D3 fill:#E1BEE7,color:#000,stroke:#BA68C8,stroke-width:1px
    style D4 fill:#E1BEE7,color:#000,stroke:#BA68C8,stroke-width:1px

    style E1 fill:#F8BBD0,color:#000,stroke:#F06292,stroke-width:1px
    style E2 fill:#F8BBD0,color:#000,stroke:#F06292,stroke-width:1px
    style E3 fill:#F8BBD0,color:#000,stroke:#F06292,stroke-width:1px
    style E4 fill:#F8BBD0,color:#000,stroke:#F06292,stroke-width:1px
    style E5 fill:#F8BBD0,color:#000,stroke:#F06292,stroke-width:1px
    
   
    linkStyle 0 stroke:#666,color;
    linkStyle 1 stroke:#666,color;
    linkStyle 2 stroke:#666,color;
    linkStyle 3 stroke:#666,color;   
    linkStyle 4 stroke:#666,color;
    linkStyle 5 stroke:#666,color;
    linkStyle 6 stroke:#666,color;
    linkStyle 7 stroke:#666,color;  
    linkStyle 8 stroke:#666,color;
    linkStyle 9 stroke:#666,color;
    linkStyle 10 stroke:#666,color;
    linkStyle 11 stroke:#666,color;
    linkStyle 12 stroke:#666,color;
    linkStyle 13 stroke:#666,color;
    linkStyle 14 stroke:#666,color;
    linkStyle 15 stroke:#666,color;   
    linkStyle 16 stroke:#666,color; 

```

<center style="font-size:14px;color:#C0C0C0;">å›¾2 pytorchå…¸å‹MLä»£ç æµç¨‹å›¾</center>

> åœ°å›¾æœ‰äº†ï¼Œå°±å¯ä»¥æ„‰å¿«çš„è§£æ„ä»£ç ä¸€ä¸€æ˜ å°„å•¦[^2]ğŸ«¡

[^2]: https://github.com/junaidaliop/pytorch-fashionMNIST-tutorial

---

=== "ä¾èµ–åº“å¯¼å…¥ä¸åŸºæœ¬é…ç½®"

    > å·§å¦‡éš¾ä¸ºæ— ç±³ä¹‹ç‚Šï¼Œä¸€ä¸ªé¡¹ç›®å¾€å¾€éœ€è¦æ•°ä¸ªåŠŸèƒ½å„å¼‚çš„åº“ååŒåˆä½œ

    **å¯¼å…¥å¿…è¦çš„åº“**

    - `torch`ï¼šPyTorch çš„æ ¸å¿ƒåº“ï¼Œç”¨äºå¼ é‡æ“ä½œå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹
    - `torch.nn`ï¼šPyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼ŒåŒ…å«å±‚å’ŒæŸå¤±å‡½æ•°
    - `torch.nn.functional`ï¼šåŒ…å«æ¿€æ´»å‡½æ•°ç­‰æ“ä½œ
    - `torch.optim`ï¼šä¼˜åŒ–ç®—æ³•æ¨¡å—ï¼Œå¦‚ SGD å’Œ Adam
    - `torchvision`ï¼šç”¨äºå¤„ç†å›¾åƒæ•°æ®é›†å’Œå›¾åƒå˜æ¢
    - `DataLoader` å’Œ `random_split`ï¼šç”¨äºåŠ è½½å’Œåˆ’åˆ†æ•°æ®é›†
    - `einops`ï¼šç”¨äºå¼ é‡æ“ä½œçš„é«˜çº§åº“
    - `matplotlib.pyplot`ï¼šç”¨äºç»˜åˆ¶å›¾è¡¨
    - `time`ï¼šç”¨äºè®¡ç®—æ—¶é—´

    ```python title="python"
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torchvision import datasets, transforms
    from torch.utils.data import DataLoader, random_split
    from einops import rearrange
    import matplotlib.pyplot as plt
    import time
    ```

    **è®¾ç½®éšæœºç§å­**

    è®¾ç½®éšæœºç§å­ä¸º 99ï¼Œç¡®ä¿~~ä»£ç å¤æ´»~~æ¯æ¬¡è¿è¡Œä»£ç æ—¶ç»“æœå¯é‡å¤ã€‚

    ```python title="python"
    # å›ºå®šéšæœºç§å­
    torch.manual_seed(99)
    ```

    **è®¾å¤‡é…ç½®**

    è¿™ä¸€æ­¥æ˜¯å¿…è¦çš„ï¼ŒAå¡å°±è€è€å®å®ç”¨CPUå§

    ```python title="python"
    # è·å–è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Using {} device".format(device))
    ```


=== "æ•°æ®é›†å¯¼å…¥ä¸å¤„ç†"

    åœ¨ä»‹ç»æ•°æ®å¤„ç†å‰ï¼Œå…ˆç®€è¦è¯´æ˜ä¸€ä¸‹å¤§åé¼é¼çš„`Fashion MNIST`æ•°æ®é›†ã€‚

    Fashion MNIST æ•°æ®é›†æ˜¯ 10 ä¸ªæ—¶å°šç±»åˆ«çš„ç°åº¦å›¾åƒé›†åˆï¼Œæ¯ä¸ªå›¾åƒå¤§å°ä¸º ==28x28== åƒç´ ã€‚å®ƒç”¨ä½œç»å…¸ MNIST æ•°æ®é›†çš„æ›¿ä»£å“ã€‚ç”±äºæœè£…é¡¹ç›®ç›¸ä¼¼ï¼Œå› æ­¤å®ƒçš„åˆ†ç±»é—®é¢˜æ¯”å¸¸è§„ MNIST æ•°å­—æ•°æ®é›†æ›´å…·æŒ‘æˆ˜æ€§ã€‚

    <center>
    
    <img src="https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png" alt="Fashion MNIST Sample" style="zoom: 50%;" />

    </center>
    <center style="font-size:14px;color:#C0C0C0;">å›¾3  æ•°æ®é›†æ¦‚è§ˆ|å›¾æºç½‘ç»œ</center>

    æ•°æ®é›†ä¸­æ¯å¹…å›¾åƒå¯¹åº”ä¸€ä¸ª0~9çš„æ ‡ç­¾ï¼Œä»£è¡¨åä¸ªç±»åˆ«ï¼š

    <center>

    | Label | Description |
    | ----- | ----------- |
    | 0     | T-shirt/top |
    | 1     | Trouser     |
    | 2     | Pullover    |
    | 3     | Dress       |
    | 4     | Coat        |
    | 5     | Sandal      |
    | 6     | Shirt       |
    | 7     | Sneaker     |
    | 8     | Bag         |
    | 9     | Ankle boot  |

    </center>
    <center style="font-size:14px;color:#C0C0C0;">è¡¨1  Fashion MNISTæ•°æ®é›†ç±»åˆ«</center>

    **æ•°æ®é¢„å¤„ç†**

    - `ToTensor()`ï¼šå°†å›¾åƒè½¬æ¢ä¸º PyTorch å¼ é‡
    - `Normalize((0.5,), (0.5,))`ï¼šå°†åƒç´ å€¼ä» [0, 1] å½’ä¸€åŒ–åˆ° [-1, 1]

    ```python title="python"
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))  # å½’ä¸€åŒ–åˆ°[-1, 1]
    ])
    ```

    **ä¸‹è½½æ•°æ®é›†**

    - `root='./data'`ï¼šæ•°æ®é›†å­˜å‚¨è·¯å¾„
    - `train=True`ï¼šåŠ è½½è®­ç»ƒé›†|`train=False`ï¼šåŠ è½½æµ‹è¯•é›†
    - `download=True`ï¼šå¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œåˆ™è‡ªåŠ¨ä¸‹è½½
    - `transform=transform`ï¼šåº”ç”¨å®šä¹‰çš„æ•°æ®é¢„å¤„ç†

    ```python title="python"
    train_dataset = datasets.FashionMNIST(
        root='./data', 
        train=True, 
        download=True, 
        transform=transform
    )
    test_dataset = datasets.FashionMNIST(
        root='./data', 
        train=False, 
        download=True, 
        transform=transform
    )
    ```

    **æ•°æ®é›†åˆ’åˆ†**

    å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼š

    - `train_size`ï¼šè®­ç»ƒé›†å¤§å°ï¼ˆ80%ï¼‰
    - `val_size`ï¼šéªŒè¯é›†å¤§å°ï¼ˆ20%ï¼‰
    - `random_split`ï¼šéšæœºåˆ’åˆ†æ•°æ®é›†

    ```python title="python"
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])
    ```

    **åˆ›å»ºDataLoader**

    - `batch_size=64`ï¼šæ¯æ¬¡åŠ è½½ 64 å¼ å›¾ç‰‡
    - `shuffle=True`ï¼šè®­ç»ƒé›†æ‰“ä¹±é¡ºåºï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä¸æ‰“ä¹±

    ```python title="python"
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    ```
    !!! tip "é¢˜å¤–è¯â€”â€”æ•°æ®å¤„ç†ç”¨åˆ°çš„åº“"

        `torchvision`æ˜¯ PyTorch ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªåº“ï¼Œå®ƒæä¾›äº†ä¸€å¥—ç”¨äºè®¡ç®—æœºè§†è§‰çš„å®ç”¨ç¨‹åºã€‚å®ƒæä¾›ï¼š
        
        - é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ ResNetã€VGG å’Œ AlexNetï¼‰
        - æµè¡Œçš„æ•°æ®é›†å’Œç”¨äºé¢„å¤„ç†è¿™äº›æ•°æ®é›†çš„è½¬æ¢
        - ç”¨äºåˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨çš„å®ç”¨ç¨‹åº
        
        æˆ‘ä»¬ä¸»è¦ç”¨äº`torchvision`è®¿é—® Fashion MNIST æ•°æ®é›†å¹¶å¯¹å›¾åƒåº”ç”¨è½¬æ¢
        
        ---
        
        `torch.utils`æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªå®ç”¨æ¨¡å—ï¼Œå®ƒæä¾›äº†å¤šä¸ªå­æ¨¡å—æ¥ååŠ©å®Œæˆå„ç§ä»»åŠ¡ã€‚
        
        å…¶æœ€å¸¸ç”¨çš„å­æ¨¡å—ä¹‹ä¸€æ˜¯`data`ï¼Œå®ƒæœ‰åŠ©äºå¤„ç†æ•°æ®é›†å¹¶æä¾›å·¥å…·æ¥é«˜æ•ˆåœ°åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€‚
        
        ---
        
        `DataLoader` `torch.utils.data`æ˜¯ä¸€ä¸ªåŒ…è£…æ•°æ®é›†å¹¶æä¾›å°æ‰¹é‡æ•°æ®çš„ç±»ã€‚å®ƒæä¾›ï¼š
        
        - æ•°æ®åˆ†æ‰¹ï¼šä¸ºäº†æ›´å¥½åœ°ä¼˜åŒ–å’Œæ›´å¿«åœ°è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬ç»å¸¸å¯¹å°æ‰¹é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒ
        - æ··æ´—ï¼šåœ¨æ¯ä¸ªæ—¶æœŸå¼€å§‹æ—¶éšæœºåœ°é‡æ–°æ’åºè®­ç»ƒæ•°æ®ï¼Œä»¥å‡å°‘æ¨¡å‹æ–¹å·®
        - å¹¶è¡ŒåŠ è½½ï¼šä½¿ç”¨å¤šçº¿ç¨‹åœ¨åå°å‡†å¤‡æ‰¹æ¬¡ï¼Œç¡®ä¿ GPU/CPU ä¿æŒç¹å¿™
        
        > è¯´äººè¯ï¼Œæ´—ç‰Œçš„ğŸ¤ª

=== "æ¨¡å‹åˆ›å»ºä¸å®ä¾‹åŒ–"

    ä»¥æœ€å…¸å‹çš„CNNä¸ºä¾‹ï¼Œä¸æ¶‰åŠDropoutã€‚å·¥ä½œæµç¨‹åœ¨[ä¸Šä¸€èŠ‚](#_2)å·²æœ‰æ‰€è¯´æ˜ã€‚

    **å®šä¹‰CNNæ¨¡å‹ç±»**

    - `conv1`ï¼šç¬¬ä¸€å±‚å·ç§¯ï¼Œè¾“å…¥é€šé“ 1ï¼Œè¾“å‡ºé€šé“ 32ï¼Œå·ç§¯æ ¸å¤§å° 3x3
    - `conv2`ï¼šç¬¬äºŒå±‚å·ç§¯ï¼Œè¾“å…¥é€šé“ 32ï¼Œè¾“å‡ºé€šé“ 64ï¼Œå·ç§¯æ ¸å¤§å° 3x3
    - `fc1`ï¼šå…¨è¿æ¥å±‚ï¼Œè¾“å…¥å¤§å°ä¸º 64*7*7ï¼Œè¾“å‡ºå¤§å°ä¸º 128
    - `fc2`ï¼šå…¨è¿æ¥å±‚ï¼Œè¾“å…¥å¤§å°ä¸º 128ï¼Œè¾“å‡ºå¤§å°ä¸º 10ï¼ˆå¯¹åº” 10 ä¸ªç±»åˆ«ï¼‰

    ```python title="python"
    class CNN(nn.Module):
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
            self.fc1 = nn.Linear(64 * 7 * 7, 128)
            self.fc2 = nn.Linear(128, 10)
    ```

    **å‰å‘ä¼ æ’­**

    æœ‰ä»¥ä¸‹è¿‡ç¨‹ï¼š

    - é€šè¿‡å·ç§¯å±‚å’Œ ReLU æ¿€æ´»å‡½æ•°æå–ç‰¹å¾
    - é€šè¿‡æœ€å¤§æ± åŒ–å±‚ä¸‹é‡‡æ ·
    - å°†ç‰¹å¾å±•å¹³ä¸ºä¸€ç»´å‘é‡
    - é€šè¿‡å…¨è¿æ¥å±‚å’Œ ReLU æ¿€æ´»å‡½æ•°è¿›è¡Œåˆ†ç±»
    - è¿”å›æœ€ç»ˆçš„è¾“å‡º

    ```python title="python"
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
    ```

    **æ¨¡å‹å®ä¾‹åŒ–**

    å³åˆ›å»ºæ¨¡å‹çš„ä¸€ä¸ªå®ä¾‹å¹¶å°†å…¶ä¼ è¾“åˆ°é€‚å½“çš„è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰

    ```python title="python"
    model = CNN().to(device)
    ```

    **ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°**

    - ä¼˜åŒ–å™¨ï¼šAdam ä¼˜åŒ–å™¨æ˜¯å¸¸ç”¨çš„ï¼Œå¯åœ¨è®­ç»ƒæœŸé—´è°ƒæ•´å­¦ä¹ ç‡
    - æŸå¤±å‡½æ•°ï¼šç”±äºè¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µæŸå¤±

    ```python title="python"
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    ```

    !!! tip "é¢˜å¤–è¯â€”â€”ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°çš„é€‰æ‹©"

        **ä¼˜åŒ–å™¨**

        ä¼˜åŒ–å™¨æ˜¯æ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥è°ƒæ•´æ¨¡å‹æƒé‡çš„ç®—æ³•ã€‚ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±ã€‚æœ‰å‡ ç§å¯ç”¨çš„ä¼˜åŒ–å™¨ï¼š

        1. **SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰**ï¼šè¿™æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•çš„åŸºæœ¬å½¢å¼ã€‚å®ƒä½¿ç”¨æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªæƒé‡çš„æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹çš„æƒé‡ã€‚
        - ä½¿ç”¨ï¼š`torch.optim.SGD(model.parameters(), lr=learning_rate)`
        2. **åŠ¨é‡**ï¼šSGD çš„ä¸€ç§å˜ä½“ï¼Œå®ƒè€ƒè™‘åˆ°äº†å‰é¢çš„æ­¥éª¤ï¼Œæœ‰åŠ©äºåŠ é€Ÿæ”¶æ•›å¹¶é¿å…å±€éƒ¨æœ€å°å€¼ã€‚
        - ä½¿ç”¨ï¼š`torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)`
        3. **Adam**ï¼šç»“åˆäº† SGD çš„å¦å¤–ä¸¤ä¸ªæ‰©å±•ï¼Œå³ AdaGrad å’Œ RMSProp çš„ä¼˜ç‚¹ã€‚å®ƒæ ¹æ®å†å²æ¢¯åº¦ä¿¡æ¯è°ƒæ•´æ¯ä¸ªæƒé‡çš„å­¦ä¹ ç‡ã€‚
        - ä½¿ç”¨ï¼š`torch.optim.Adam(model.parameters(), lr=learning_rate)`
        4. **RMSProp**ï¼šä¿æŒæ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡å€¼ï¼Œå¹¶å°†æ¢¯åº¦é™¤ä»¥è¯¥å¹³å‡å€¼çš„æ ¹ã€‚
        - ä½¿ç”¨ï¼š`torch.optim.RMSprop(model.parameters(), lr=learning_rate)`

        ---

        **æŸå¤±å‡½æ•°**

        1. **äº¤å‰ç†µæŸå¤±**ï¼šç”¨äºå¤šç±»åˆ†ç±»ã€‚å®ƒé‡åŒ–äº†é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå’Œå®é™…åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚
        - ä½¿ç”¨ï¼š`nn.CrossEntropyLoss()`
        2. **äºŒå…ƒäº¤å‰ç†µæŸå¤±**ï¼šä¸“é—¨ç”¨äºäºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€‚
        - ä½¿ç”¨ï¼š`nn.BCELoss()`
        3. **Hinge Lossï¼ˆæˆ– Margin Lossï¼‰**ï¼šç”¨äºâ€œæœ€å¤§è¾¹ç¼˜â€åˆ†ç±»ï¼Œä¸»è¦ç”¨äº SVMã€‚
        - ä½¿ç”¨ï¼š`nn.HingeEmbeddingLoss()`

        > æ ¹æ®æ—¢å®šä»»åŠ¡å› åœ°åˆ¶å®œ

=== "æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°"

    **è®­ç»ƒ**

    è®­ç»ƒç¥ç»ç½‘ç»œæ¶‰åŠè¿­ä»£æ›´æ–°å…¶æƒé‡ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚æ­¤è¿‡ç¨‹é€šå¸¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•æ¥å®ç°ã€‚

    ä¸€èˆ¬åŒ…å«ï¼š

    - **Epochs**ï¼šä¸€ä¸ªepochä»£è¡¨æ‰€æœ‰è®­ç»ƒæ ·ä¾‹çš„ä¸€æ¬¡å®Œæ•´çš„å‰å‘å’Œåå‘ä¼ é€’ã€‚å‘¨æœŸæ•° (num_epochs) æ˜¯å­¦ä¹ ç®—æ³•éå†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ¬¡æ•°ã€‚é€šå¸¸æ˜¯è‡ªå®šä¹‰è¶…å‚æ•°
    - **æ¨¡å‹è®­ç»ƒæ¨¡å¼**ï¼šç¥ç»ç½‘ç»œå¯ä»¥ä»¥ä¸åŒçš„æ¨¡å¼è¿è¡Œâ€”â€”è®­ç»ƒå’Œè¯„ä¼°
    - **æ‰¹å¤„ç†**ï¼šæˆ‘ä»¬é€šå¸¸åœ¨ä¸€ç»„ç§°ä¸ºæ‰¹æ¬¡çš„è®­ç»ƒç¤ºä¾‹ä¹‹åæ›´æ–°æƒé‡ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ªè®­ç»ƒç¤ºä¾‹ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰æˆ–æ•´ä¸ªæ•°æ®é›†ï¼ˆæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼‰ä¹‹åæ›´æ–°æƒé‡
    - **æ¢¯åº¦å½’é›¶**ï¼šåœ¨ PyTorch ä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹æ¢¯åº¦ä¼šç´¯ç§¯ã€‚åœ¨è®¡ç®—å½“å‰æ‰¹æ¬¡ä¸­çš„æ–°æ¢¯åº¦ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†å…ˆå‰çš„æ¢¯åº¦è®¾ç½®ä¸ºé›¶
    - **å‰å‘ä¼ é€’**ï¼šè¾“å…¥æ•°æ®ï¼ˆå›¾åƒï¼‰é€šè¿‡ç½‘ç»œé€å±‚ä¼ é€’ï¼Œç›´åˆ°å¾—åˆ°è¾“å‡ºã€‚è¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºå‰å‘ä¼ æ’­
    - **è®¡ç®—æŸå¤±**ï¼šä¸€æ—¦æˆ‘ä»¬æœ‰äº†ç½‘ç»œçš„é¢„æµ‹ï¼ˆè¾“å‡ºï¼‰ï¼Œæˆ‘ä»¬å°±ä½¿ç”¨æŸå¤±å‡½æ•°å°†å®ƒä»¬ä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚è¿™å¯ä»¥è¡¡é‡ç½‘ç»œçš„é¢„æµ‹ä¸å®é™…æ ‡ç­¾çš„åŒ¹é…ç¨‹åº¦
    - **åå‘ä¼ æ’­**ï¼šä¸ºäº†æ›´æ–°æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¯ä¸ªæƒé‡çš„æ¢¯åº¦ã€‚å‘åä¼ é€’è®¡ç®—è¿™äº›æ¢¯åº¦
    - **æ›´æ–°æƒé‡**ï¼šä¼˜åŒ–å™¨æ ¹æ®å‘åä¼ é€’ä¸­è®¡ç®—çš„æ¢¯åº¦æ›´æ–°æƒé‡

    å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡é‡å¤æ­¤å¾ªç¯ï¼ˆå‰å‘ä¼ é€’ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ é€’ã€æƒé‡æ›´æ–°ï¼‰ã€‚



    ```python title="python"
    # è¿­ä»£æ¬¡æ•°
    num_epochs = 1

    # Start the training loop
    for epoch in range(num_epochs):
        # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        size = len(trainloader.dataset)
        # è¿­ä»£æ¯æ‰¹è®­ç»ƒæ•°æ®
        for batch, (images, labels) in enumerate(trainloader):
            # å°†å›¾åƒå’Œæ ‡ç­¾ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰
            images, labels = images.to(device), labels.to(device)   
            # æ¸…é™¤ä¸Šä¸€æ¬¡è¿­ä»£çš„æ¢¯åº¦
            optimizer.zero_grad()  
            # å‰å‘ä¼ é€’ï¼šå°†å›¾åƒä¼ é€’ç»™æ¨¡å‹ä»¥è·å¾—é¢„æµ‹è¾“å‡º
            outputs = model(images)
            # è®¡ç®—é¢„æµ‹è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„æŸå¤±
            loss = criterion(outputs, labels)
            # å‘åä¼ é€’ï¼šè®¡ç®—æŸå¤± w.r.t çš„æ¢¯åº¦ã€‚æ¨¡å‹å‚æ•°
            loss.backward() 
            # æ›´æ–°æ¨¡å‹å‚æ•°
            optimizer.step()
            # Print the loss every 100 batches
            if batch % 100 == 0:
                loss, current = loss.item(), batch * len(images)
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
    ```

    **è¯„ä¼°ï¼ˆæµ‹è¯•ï¼‰**

    ä¸€æ—¦æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯„ä¼°å…¶åœ¨æœªçŸ¥æ•°æ®ä¸Šçš„è¡¨ç°å°±è‡³å…³é‡è¦ã€‚

    - ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°å‡†ç¡®åº¦ä¸æ³›ç”¨æ€§
    - æ‰“å°æŠ¥å‘Šä¸ç»˜åˆ¶ç»“æœï¼ˆå¦‚æœä½ æ„¿æ„çš„è¯ï¼‰

    ```python title="python"
    import numpy as np
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    # ç”¨äºå­˜å‚¨æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„åˆ—è¡¨
    all_preds = []
    all_labels = []
    # ä½¿ç”¨ torch.no_grad() è®¡ç®—æ¢¯åº¦
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            # å°†å›¾åƒä¼ é€’ç»™æ¨¡å‹ä»¥è·å–é¢„æµ‹
            outputs = model(images)
            # è·å–å…·æœ‰æœ€å¤§æ¦‚ç‡çš„ç±»ä½œä¸ºé¢„æµ‹ç±»
            _, predicted = torch.max(outputs, 1)
            # ä½¿ç”¨çœŸå®æ ‡ç­¾å’Œé¢„æµ‹è®¡ç®—æ··æ·†çŸ©é˜µ
            all_preds.extend(predicted.cpu().numpy())
            # ä½¿ç”¨æ­¤æ‰¹æ¬¡ä¸­çš„çœŸå®æ ‡ç­¾æ‰©å±•all_labelsåˆ—è¡¨
            all_labels.extend(labels.cpu().numpy())
    # æ‰“å°æŠ¥å‘Š
    # ç»˜åˆ¶ç»“æœ
    ```

## Putting CNN to Work

> å¤šè¯´æ— ç›Šï¼Œåœ¨ç”µè„‘é£æ‰‡å‘¼å•¸ä¹‹å‰ï¼Œä½ æ°¸è¿œæ— æ³•çœŸåˆ‡æ„Ÿå—åˆ°MLçš„é­…åŠ›ï¼

!!! info "MY ENVIRONMENT"

     1. **Anaconda**ï¼š[ç¯å¢ƒç®¡ç†å™¨](https://anaconda.org/)ï¼Œbaseå…«å®ç²¥ç ´åè€…
     2. **Python**ï¼šæœ€å¥½æ˜¯Python 3.x
     3. **PyTorch å’Œ torchvision**ï¼š[PyTorch](https://pytorch.org/) æ˜¯ä¸€ä¸ªå¼€æºæœºå™¨å­¦ä¹ åº“ï¼Œtorchvision æä¾›è®¡ç®—æœºè§†è§‰çš„æ•°æ®é›†å’Œæ¨¡å‹
     4. **Jupyter Notebook**ï¼š[äº¤äº’å¼ç¯å¢ƒ](https://jupyter.org/)ï¼ŒåŸºäºVscodeç¼–è¯‘å™¨
     5. **NumPy**ï¼šPython ä¸­çš„æ•°å€¼è¿ç®—åº“
     6. **scikit-learn**ï¼šPython ä¸­çš„æœºå™¨å­¦ä¹ åº“ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥è·å–æ€§èƒ½æŒ‡æ ‡
     7. **Seaborn å’Œ Matplotlib**ï¼šPython ä¸­çš„å¯è§†åŒ–åº“
     8. **CUDAï¼ˆå¯é€‰ï¼‰**ï¼šå¦‚æœæ‚¨æœ‰å…¼å®¹çš„ NVIDIA GPUï¼Œåˆ™å¯ä»¥å®‰è£… CUDA ä»¥å®ç° PyTorch çš„ GPU åŠ é€Ÿ

!!! tip "å¯èƒ½éœ€è¦çš„ï¼š"

     - [Anaconda + Pytorch è¶…è¯¦ç»†å®‰è£…æ•™ç¨‹](https://blog.csdn.net/qq_45057249/article/details/130438318)
     - [condaå¸¸ç”¨å‘½ä»¤ï¼šä»å…¥é—¨åˆ°å…¥åœŸ](https://blog.csdn.net/chenxy_bwave/article/details/119996001)
     - å¯¹äº`pip/pipx`ä¸‹è½½ï¼Œè‹¥é‡åˆ°ä¸‹è½½é€Ÿåº¦è¿‡æ…¢ï¼Œä¸å¦¨ç»™å®šé•œåƒæºï¼ˆé€šå¸¸æ˜¯æ¸…å/é˜¿é‡Œï¼‰ï¼Œå³

     ```powershell title="powershell"
     pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple
     ```

     - éå¤§é¡¹ç›®ï¼Œå»ºè®®è½¬æˆ˜`Vscode`ï¼ŒALL IN ONEå®ƒä¸é¦™å˜›ğŸ˜œ

---

### RUNNINGï¼

æœ¬èŠ‚å°†ä»ä¸¤ä¸ªæ–¹é¢ ~~æ‹·æ‰“~~ æµ‹è¯•CNN

- æ§åˆ¶æ¨¡å‹ï¼Œæ”¹å˜è¶…å‚ï¼ˆå­¦ä¹ ç‡lrã€ä¼˜åŒ–å™¨ã€æ¿€æ´»å‡½æ•°ï¼‰

  ```python title="python"
  # å®šä¹‰ä¸åŒçš„å‚æ•°ç»„åˆ
  learning_rates = [0.001, 0.01, 0.1]
  optimizers = [optim.Adam, optim.SGD]
  activation_functions = [F.relu, F.leaky_relu]
  ```

- æ§åˆ¶è¶…å‚ï¼Œæ”¹å˜æ¨¡å‹

  ```python title="python"
  # å®šä¹‰æ¨¡å‹åˆ—è¡¨
  models = {
      'CNN': CNN().to(device),
      'ResNet': ResNet().to(device),
      'ViT': ViT().to(device)
  }
  ```

=== "The Impact of Hyperparameters on Model Performance"

    **è®­ç»ƒä¸æµ‹è¯•ç»“æœ**

    <center>

    ![](https://cdn.jsdelivr.net/gh/dixiLOG/blogStatic/CNN_25_1.png){style="zoom:60%;"}

    </center>

    <center style="font-size:14px;color:#C0C0C0;">å›¾4 ä¸åŒè¶…å‚ä¸‹çš„æ¨¡å‹æ€§èƒ½ç»“æœå¯¹æ¯”</center>

    <center>

    | Learning Rate | Optimizer | Activation Function | Training Time (s) | Inference Time (ms) | Test Loss | Test Accuracy | Parameters | Model Size (Bytes) |
    | :-----------: | :-------: | :-----------------: | :---------------: | :-----------------: | :-------: | :-----------: | :--------: | :----------------: |
    |     0.001     |   Adam    |        relu         |      241.59       |        0.103        |  0.3053   |    0.9134     |   421642   |      1686568       |
    |     0.001     |   Adam    |     leaky_relu      |      240.98       |        0.106        |  0.2808   |    0.9161     |   421642   |      1686568       |
    |     0.001     |    SGD    |        relu         |      244.02       |        0.105        |  0.5782   |    0.7879     |   421642   |      1686568       |
    |     0.001     |    SGD    |     leaky_relu      |      245.92       |        0.107        |  0.5764   |    0.7905     |   421642   |      1686568       |
    |     0.01      |   Adam    |        relu         |      253.14       |        0.114        |  0.3572   |    0.8729     |   421642   |      1686568       |
    |     0.01      |   Adam    |     leaky_relu      |      267.85       |        0.115        |  0.3834   |    0.8640     |   421642   |      1686568       |
    |     0.01      |    SGD    |        relu         |      277.47       |        0.122        |  0.3529   |    0.8718     |   421642   |      1686568       |
    |     0.01      |    SGD    |     leaky_relu      |      273.44       |        0.119        |  0.3508   |    0.8704     |   421642   |      1686568       |
    |      0.1      |   Adam    |        relu         |      278.61       |        0.123        |  2.3124   |    0.1000     |   421642   |      1686568       |
    |      0.1      |   Adam    |     leaky_relu      |      258.18       |        0.105        |  2.3089   |    0.1000     |   421642   |      1686568       |
    |      0.1      |    SGD    |        relu         |      249.44       |        0.117        |  0.2546   |    0.9131     |   421642   |      1686568       |
    |      0.1      |    SGD    |     leaky_relu      |      252.32       |        0.110        |  0.2749   |    0.9087     |   421642   |      1686568       |

    </center>

    <center style="font-size:14px;color:#C0C0C0;">è¡¨2 ä¸åŒè¶…å‚ä¸‹çš„æ¨¡å‹æµ‹è¯•ç»“æœä¸å¤æ‚åº¦è®¡ç®—</center>


    **ä¸åŒå­¦ä¹ ç‡å¯¹Lossçš„å½±å“**

    **ä½å­¦ä¹ ç‡ï¼ˆå¦‚0.001ï¼‰**ï¼š

    - ä½å­¦ä¹ ç‡ä½¿æ¨¡å‹æ¯æ¬¡æ›´æ–°çš„æ­¥å¹…è¾ƒå°ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹è¾ƒç¨³å®šï¼Œä½†éœ€è¦æ›´å¤šè¿­ä»£æ¬¡æ•°æ‰èƒ½æ”¶æ•›
    - ä¼˜åŠ¿ï¼šèƒ½é¿å…æ¢¯åº¦çˆ†ç‚¸æˆ–ä¸ç¨³å®šæ”¶æ•›
    - ç¼ºç‚¹ï¼šè®­ç»ƒæ—¶é—´é•¿

    **é«˜å­¦ä¹ ç‡ï¼ˆå¦‚0.1ï¼‰**ï¼š

    - é«˜å­¦ä¹ ç‡ä½¿æ¯æ¬¡æƒé‡æ›´æ–°å¹…åº¦è¾ƒå¤§ï¼Œå¯èƒ½å¯¼è‡´Lossåœ¨æœ€ä¼˜å€¼é™„è¿‘æŒ¯è¡ç”šè‡³å‘æ•£
    - å®éªŒä¸­çš„éªŒè¯é›†Lossè¾ƒå¤§æ³¢åŠ¨ï¼Œè¯´æ˜æ¨¡å‹æœªèƒ½æœ‰æ•ˆå­¦ä¹ ç›®æ ‡å‡½æ•°

    **æœ€ä½³å­¦ä¹ ç‡ï¼ˆ0.001ï¼‰**ï¼š

    - ç¨³å®šä¸‹é™ä¸”æœ€ç»ˆLossæœ€ä½ï¼Œè¯æ˜å…¶å…¼é¡¾äº†æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚

    ------

    **ä¸åŒä¼˜åŒ–å™¨å¯¹Lossçš„å½±å“**

    **Adamä¼˜åŒ–å™¨**ï¼š

    - Adamç»“åˆäº†åŠ¨é‡ä¸è‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜ç‚¹ï¼Œèƒ½è‡ªåŠ¨è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ï¼Œç‰¹åˆ«åœ¨éå‡¸ä¼˜åŒ–é—®é¢˜ä¸­è¡¨ç°è‰¯å¥½
    - å®éªŒä¸­ï¼ŒAdamæ”¶æ•›é€Ÿåº¦å¿«ï¼ŒLossæ›²çº¿é™¡å³­ä¸‹é™ï¼ŒéªŒè¯é›†Lossä¹Ÿè¾ƒä½
    - åœ¨é«˜å­¦ä¹ ç‡æ—¶å´©åï¼Œä¸èƒ½ä½¿ç”¨

    **SGDä¼˜åŒ–å™¨**ï¼š

    - SGDé‡‡ç”¨å›ºå®šå­¦ä¹ ç‡ï¼Œä¼˜åŒ–è¿‡ç¨‹ä¾èµ–äºæ¢¯åº¦çš„å…¨å±€æ–¹å‘ã€‚
    - åœ¨å®éªŒä¸­ï¼ŒSGDè¡¨ç°å‡ºæ”¶æ•›æ…¢çš„ç‰¹æ€§ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œä½†åœ¨é«˜lræ—¶è¡¨ç°å‡ºæ›´ç¨³å®šæ›´ä¼˜ç§€çš„ç‰¹æ€§ã€‚

    ------

    **ä¸åŒæ¿€æ´»å‡½æ•°å¯¹Lossçš„å½±å“**

    **ReLUæ¿€æ´»å‡½æ•°**ï¼š

    - ReLUå¯¹æ­£å€¼ä¿æŒçº¿æ€§ï¼Œè€Œå¯¹è´Ÿå€¼è¾“å‡º0ï¼Œè®¡ç®—é«˜æ•ˆä¸”æ”¶æ•›é€Ÿåº¦å¿«
    - å®éªŒä¸­ï¼ŒReLUçš„è®­ç»ƒå’ŒéªŒè¯Lossæ›²çº¿å¹³æ»‘ä¸‹é™ï¼Œä¸”éªŒè¯Lossè¾ƒä½ï¼Œè¡¨æ˜æ¨¡å‹å­¦åˆ°äº†æœ‰æ•ˆçš„ç‰¹å¾

    **Leaky ReLUæ¿€æ´»å‡½æ•°**ï¼š

    - Leaky ReLUåœ¨è´Ÿå€¼åŒºåŸŸæä¾›äº†å¾®å°çš„è´Ÿæ–œç‡ï¼Œé¿å…äº†â€œç¥ç»å…ƒæ­»äº¡â€çš„é—®é¢˜
    - å®éªŒä¸­ï¼ŒLeaky ReLUçš„éªŒè¯Lossåœ¨åˆæœŸä½äºReLUï¼Œä½†ä¸­åæœŸæ³¢åŠ¨è¾ƒå¤§ï¼Œå¯èƒ½å› è¾ƒå¤æ‚çš„è´Ÿå€¼æ¢¯åº¦å­¦ä¹ å¸¦æ¥ä¸ç¨³å®š

    ------


    **æµ‹è¯•Losså’Œå‡†ç¡®ç‡ï¼ˆTest Accuracyï¼‰**ï¼š

    - `lr=0.001, Adam, ReLU`çš„æµ‹è¯•Accuracyæœ€é«˜ï¼ˆ0.9134ï¼‰ï¼Œè¡¨æ˜å…¶æ³›åŒ–èƒ½åŠ›æœ€ä½³ã€‚
    - é«˜å­¦ä¹ ç‡ï¼ˆå¦‚0.1ï¼‰æˆ–ä½¿ç”¨Adamæ—¶ï¼Œæµ‹è¯•Losså’Œå‡†ç¡®ç‡ä¸‹é™ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆã€‚

    **è®­ç»ƒæ—¶é—´ä¸æ¨ç†æ—¶é—´**ï¼š

    - Adamä¼˜åŒ–å™¨éœ€è¦æ›´å¤šè®¡ç®—é‡ä»¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œå› æ­¤è®­ç»ƒæ—¶é—´ç•¥é•¿ã€‚
    - æ¨ç†æ—¶é—´ä¸æ¨¡å‹å¤§å°æ— å…³ï¼Œå› æ­¤ä¸åŒè¶…å‚æ•°é…ç½®ä¸‹å‡ ä¹ç›¸åŒã€‚

    ------

    **ä¸€äº›ç»“è®º**

    1. **å­¦ä¹ ç‡çš„å¹³è¡¡**ï¼š0.001æä¾›äº†æœ€ä½³çš„ç¨³å®šæ€§ä¸æ”¶æ•›æ•ˆæœ
    2. **ä¼˜åŒ–å™¨çš„é€‰æ‹©**ï¼šAdamè¡¨ç°æ›´å¿«ï¼Œä½†SGDåœ¨æŸäº›åœºæ™¯å¯å®ç°ç«äº‰åŠ›
    3. **æ¿€æ´»å‡½æ•°çš„å–èˆ**ï¼šReLUæ›´ç¨³å¥ï¼Œè€ŒLeaky ReLUæœ‰æ½œåœ¨ä¼˜åŠ¿ä½†æ³¢åŠ¨è¾ƒå¤§
    4. **æ³›åŒ–èƒ½åŠ›**ï¼šæµ‹è¯•é›†è¡¨ç°ä¸Šï¼Œ`lr=0.001, Adam, ReLU`æ˜¾ç„¶æ˜¯æœ€ä¼˜é€‰æ‹©



=== "Who Wears the Crown of FashionMNIST Classification"

    **è®­ç»ƒä¸æµ‹è¯•ç»“æœ**

    <center>

    ![](https://cdn.jsdelivr.net/gh/dixiLOG/blogStatic/CNN_28_1.png){style="zoom:100%;"}

    </center>

    <center style="font-size:14px;color:#C0C0C0;">å›¾5 ä¸åŒæ¨¡å‹çš„æ¨¡å‹æ€§èƒ½ç»“æœå¯¹æ¯”</center>

    | Model      | Parameters   | Model Size (Bytes) | Training Time (s) | Avg. Inference Time per Sample (ms) | Test Loss  | Test Accuracy |
    | ---------- | ------------ | ------------------ | ----------------- | ----------------------------------- | ---------- | ------------- |
    | CNN        | 421642       | 1686568            | 307.94            | 0.139                               | 0.3065     | 0.9116        |
    | **ResNet** | **11172810** | **44691240**       | **6906.87**       | **0.4027**                          | **0.3030** | **0.9189**    |
    | ViT        | 139018       | 556072             | 399.43            | 0.138                               | 2.3033     | 0.1000        |

    <center style="font-size:14px;color:#C0C0C0;">è¡¨3 ä¸åŒæ¨¡å‹ä¸‹çš„æ€§èƒ½ç»“æœå¯¹æ¯”</center>


    **è®­ç»ƒå’ŒéªŒè¯æŸå¤±åˆ†æ**

    - **CNN** å’Œ **ResNet** çš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ•´ä½“è¾ƒä½ï¼Œä¸”å˜åŒ–è¶‹åŠ¿è¾ƒä¸ºç¨³å®šã€‚éšç€ epoch å¢åŠ ï¼ŒæŸå¤±é€æ¸å‡å°ï¼Œä½† **CNN** è®­ç»ƒåæœŸå¯èƒ½å‡ºç°è½»å¾®çš„è¿‡æ‹Ÿåˆç°è±¡ï¼ˆå¦‚éªŒè¯æŸå¤±ç•¥æœ‰å¢åŠ ï¼‰ã€‚
    - **ViT** çš„è®­ç»ƒæŸå¤±ä¸éªŒè¯æŸå¤±éå¸¸é«˜ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½æ¬ æ‹Ÿåˆã€‚

    **å‚æ•°é‡ä¸æ¨¡å‹å¤§å°**

    - **CNN** å‚æ•°é‡é€‚ä¸­ï¼ˆ421,642ï¼‰ï¼Œè¡¨æ˜ CNN æ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆèµ„æºå—é™çš„è®¾å¤‡
    - **ResNet** çš„å‚æ•°é‡æ˜¾è‘—å¢åŠ ï¼ˆ11,172,810ï¼‰ï¼Œæ¨¡å‹å¤§å°æœ€å¤§ï¼ˆ~44.69 MBï¼‰ï¼Œè¡¨æ˜å®ƒæ›´å¤æ‚ï¼Œæ›´é€‚åˆå¤„ç†æ›´å¤æ‚çš„æ•°æ®æˆ–ä»»åŠ¡
    - **ViT** çš„å‚æ•°é‡æœ€å°‘ï¼ˆ139,018ï¼‰ï¼Œæ¨¡å‹å¤§å°ä¹Ÿæœ€å°ï¼ˆ~0.56 MBï¼‰ï¼Œç†è®ºä¸Šåº”è¯¥å…·å¤‡æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œä½†å…¶æ€§èƒ½å¯èƒ½å—åˆ°æ•°æ®é›†è§„æ¨¡å’Œæ¨¡å‹æ¶æ„çš„å½±å“

    **è®­ç»ƒæ—¶é—´ä¸æ¨ç†æ—¶é—´**

    - **CNN** çš„è®­ç»ƒæ—¶é—´æœ€çŸ­ï¼ˆ307.94 ç§’ï¼‰ï¼Œæ¨ç†æ—¶é—´ä¹Ÿæœ€ä½ï¼Œè¡¨æ˜å®ƒæ•ˆç‡æœ€é«˜ï¼Œéå¸¸é€‚åˆå¿«é€Ÿè¿­ä»£
    - **ResNet** çš„è®­ç»ƒæ—¶é—´è¶…çº§é•¿ï¼ˆ6,906.87 ç§’ï¼‰ï¼Œæ¨ç†æ—¶é—´ä¹Ÿç›¸å¯¹è¾ƒé«˜ï¼Œä½†å®ƒåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜å¼‚
    - **ViT** çš„è®­ç»ƒæ—¶é—´åˆ™ç•¥é•¿äº **CNN**ï¼ˆ399.43 ç§’ï¼‰

    **æµ‹è¯•æŸå¤±ä¸å‡†ç¡®ç‡**

    - **CNN** å’Œ **ResNet** åœ¨æµ‹è¯•é›†ä¸Šçš„æŸå¤±è¾ƒä½ï¼ˆåˆ†åˆ«ä¸º 0.3065 å’Œ 0.3030ï¼‰ï¼Œä¸”æµ‹è¯•å‡†ç¡®ç‡è¾ƒé«˜ï¼ˆåˆ†åˆ«ä¸º 91.16% å’Œ 91.89%ï¼‰
    - **ResNet** çš„è¡¨ç°ç•¥ä¼˜äº **CNN**
    - **ViT** çš„æµ‹è¯•æŸå¤±é«˜è¾¾ 2.3033ï¼Œæµ‹è¯•å‡†ç¡®ç‡ä»…ä¸º 10%ï¼Œè¿™çº¯çº¯åœ¨çŒœå•ŠğŸ¤£ã€‚è¿™è¡¨æ˜ **ViT** æœªèƒ½å¾ˆå¥½åœ°å­¦ä¹ åˆ° FashionMNIST çš„ç‰¹å¾

    **ç»“è®º**

    - **CNN** æ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„æ¨¡å‹ï¼Œåœ¨å‡†ç¡®ç‡å’Œæ¨ç†æ—¶é—´ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚
    - **ResNet** æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹ï¼Œåœ¨æ€§èƒ½ä¸Šç¨èƒœä¸€ç­¹ï¼Œä½†éœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´å’Œèµ„æºã€‚
    - **ViT** çš„è¡¨ç°æå·®ï¼Œå¯èƒ½æ˜¯å› ä¸º ViT å¯¹è®­ç»ƒæ•°æ®çš„è§„æ¨¡å’Œå¤šæ ·æ€§æ›´ä¸ºæ•æ„Ÿï¼Œè€Œ FashionMNIST æ•°æ®é›†è¾ƒå°ï¼Œå¯¼è‡´æ¨¡å‹æœªèƒ½å……åˆ†å‘æŒ¥å…¶ä¼˜åŠ¿ã€‚

    **==å¯¹äº FashionMNIST æ•°æ®é›†==**

    * **ResNet** æ˜¯ç»¼åˆè¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶æ¬¡æ˜¯ **CNN**

    - **ViT** åœ¨è¯¥ä»»åŠ¡ä¸­ä¸é€‚ç”¨ï¼Œä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸­å¯èƒ½æ›´æœ‰æ½œåŠ›ã€‚

    !!! abstract "è¡¥å……"

        è‹¥æœåŠ¡å™¨æ˜¯Aå¡ï¼Œå³ä¸èƒ½GPUåŠ é€Ÿï¼Œåˆ™å¾ˆæ˜æ˜¾ **ç»å…¸CNN** æ‰æ˜¯ **Champion** ã€‚ç›¸å·®ä¸å¤šçš„å‡†ç¡®ç‡ï¼Œç­‰ **ResNet** è·‘å®Œäººéƒ½éº»äº†ã€‚

        ä½†å®é™…ä¸ŠåŸºæœ¬éƒ½æ˜¯ç”¨çš„GPUï¼ŒåŸºæœ¬ä¸Šæ˜¯10å€æ•ˆç‡å¾€ä¸Šï¼Œæ‰€ä»¥ç›¸å¯¹æ¥è¯´ **ResNet** çš„æ—¶é—´æ˜¯å¯ä»¥æ¥å—çš„ï¼Œæ•…å…¶ä¸ºæœ€ä¼˜è§£

        å½“ç„¶ï¼Œ **VIT** æœ¬å°±æ˜¯æ‹¿æ¥åˆ’æ°´äº†ï¼Œè¿™ç‚¹æ ·æœ¬é‡éƒ½ä¸å¤Ÿå‰”ç‰™æ~

---

## æœ€å

è¿™æ˜¯ç¬”è€…ç¬¬ä¸€æ¬¡ä»æŸ¥é˜…èµ„æ–™åˆ°å®éªŒç»“æœåˆ†æï¼Œç³»ç»Ÿåœ°èµ°äº†ä¸€éã€‚

æœŸæœ«å‘¨åŒ†åŒ†å¿™å¿™èµ¶å‡ºæ¥çš„ï¼Œä¹Ÿæ˜¯æ¼æ´ç™¾å‡ºã€‚

ä½†æ€»ç®—æ˜¯ç»“æŸå•¦~



## é™„å½•

```python title="python"
# å¯¼å…¥å¿…è¦çš„åº“
import torch  # PyTorch æ ¸å¿ƒåº“ï¼Œç”¨äºå¼ é‡æ“ä½œå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹
import torch.nn as nn  # PyTorch çš„ç¥ç»ç½‘ç»œæ¨¡å—ï¼ŒåŒ…å«å±‚å’ŒæŸå¤±å‡½æ•°
import torch.nn.functional as F  # åŒ…å«æ¿€æ´»å‡½æ•°ç­‰æ“ä½œ
import torch.optim as optim  # ä¼˜åŒ–ç®—æ³•æ¨¡å—ï¼Œå¦‚ SGD å’Œ Adam
from torchvision import datasets, transforms  # ç”¨äºå¤„ç†å›¾åƒæ•°æ®é›†å’Œå›¾åƒå˜æ¢
from torch.utils.data import DataLoader, random_split  # ç”¨äºåŠ è½½å’Œåˆ’åˆ†æ•°æ®é›†
from einops import rearrange  # ç”¨äºå¼ é‡æ“ä½œçš„é«˜çº§åº“
import matplotlib.pyplot as plt  # ç”¨äºç»˜åˆ¶å›¾è¡¨
import time  # ç”¨äºè®¡ç®—æ—¶é—´

# å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶ç»“æœå¯é‡å¤
torch.manual_seed(99)

# è·å–è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨ GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using {} device".format(device))  # æ‰“å°å½“å‰ä½¿ç”¨çš„è®¾å¤‡

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.ToTensor(),  # å°†å›¾åƒè½¬æ¢ä¸º PyTorch å¼ é‡
    transforms.Normalize((0.5,), (0.5,))  # å°†åƒç´ å€¼ä» [0, 1] å½’ä¸€åŒ–åˆ° [-1, 1]
])

# åŠ è½½ Fashion MNIST æ•°æ®é›†
train_dataset = datasets.FashionMNIST(
    root='./data',  # æ•°æ®é›†å­˜å‚¨è·¯å¾„
    train=True,  # åŠ è½½è®­ç»ƒé›†
    download=True,  # å¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œåˆ™è‡ªåŠ¨ä¸‹è½½
    transform=transform  # åº”ç”¨å®šä¹‰çš„æ•°æ®é¢„å¤„ç†
)
test_dataset = datasets.FashionMNIST(
    root='./data',  # æ•°æ®é›†å­˜å‚¨è·¯å¾„
    train=False,  # åŠ è½½æµ‹è¯•é›†
    download=True,  # å¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œåˆ™è‡ªåŠ¨ä¸‹è½½
    transform=transform  # åº”ç”¨å®šä¹‰çš„æ•°æ®é¢„å¤„ç†
)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
train_size = int(0.8 * len(train_dataset))  # è®­ç»ƒé›†å¤§å°ï¼ˆ80%ï¼‰
val_size = len(train_dataset) - train_size  # éªŒè¯é›†å¤§å°ï¼ˆ20%ï¼‰
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])  # éšæœºåˆ’åˆ†æ•°æ®é›†

# åˆ›å»º DataLoader
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # è®­ç»ƒé›† DataLoaderï¼Œæ‰“ä¹±é¡ºåº
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)  # éªŒè¯é›† DataLoaderï¼Œä¸æ‰“ä¹±é¡ºåº
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # æµ‹è¯•é›† DataLoaderï¼Œä¸æ‰“ä¹±é¡ºåº

# æ ‡å‡† CNN æ¨¡å‹
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # ç¬¬ä¸€å±‚å·ç§¯ï¼Œè¾“å…¥é€šé“ 1ï¼Œè¾“å‡ºé€šé“ 32
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  
     	# ç¬¬äºŒå±‚å·ç§¯ï¼Œè¾“å…¥é€šé“ 32ï¼Œè¾“å‡ºé€šé“ 64
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) 
     	# å…¨è¿æ¥å±‚ï¼Œè¾“å…¥å¤§å°ä¸º 64*7*7ï¼Œè¾“å‡ºå¤§å°ä¸º 128	
        self.fc1 = nn.Linear(64 * 7 * 7, 128) 
        # å…¨è¿æ¥å±‚ï¼Œè¾“å…¥å¤§å°ä¸º 128ï¼Œè¾“å‡ºå¤§å°ä¸º 10ï¼ˆå¯¹åº” 10 ä¸ªç±»åˆ«ï¼‰
        self.fc2 = nn.Linear(128, 10)  

    def forward(self, x):
        x = F.relu(self.conv1(x))  # é€šè¿‡ç¬¬ä¸€å±‚å·ç§¯å’Œ ReLU æ¿€æ´»å‡½æ•°
        x = F.max_pool2d(x, 2)  # é€šè¿‡æœ€å¤§æ± åŒ–å±‚ä¸‹é‡‡æ ·
        x = F.relu(self.conv2(x))  # é€šè¿‡ç¬¬äºŒå±‚å·ç§¯å’Œ ReLU æ¿€æ´»å‡½æ•°
        x = F.max_pool2d(x, 2)  # é€šè¿‡æœ€å¤§æ± åŒ–å±‚ä¸‹é‡‡æ ·
        x = x.view(x.size(0), -1)  # å°†ç‰¹å¾å±•å¹³ä¸ºä¸€ç»´å‘é‡
        x = F.relu(self.fc1(x))  # é€šè¿‡å…¨è¿æ¥å±‚å’Œ ReLU æ¿€æ´»å‡½æ•°
        x = self.fc2(x)  # é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»
        return x  # è¿”å›æœ€ç»ˆçš„è¾“å‡º

# ResNet æ¨¡å‹
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)  # ç¬¬ä¸€å±‚å·ç§¯
        self.bn1 = nn.BatchNorm2d(out_channels)  # æ‰¹å½’ä¸€åŒ–å±‚
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)  # ç¬¬äºŒå±‚å·ç§¯
        self.bn2 = nn.BatchNorm2d(out_channels)  # æ‰¹å½’ä¸€åŒ–å±‚

        self.shortcut = nn.Sequential()  # å¿«æ·è¿æ¥
        if stride != 1 or in_channels != out_channels:  # å¦‚æœæ­¥é•¿ä¸ä¸º 1 æˆ–è¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒ
            self.shortcut = nn.Sequential(
                # 1x1 å·ç§¯è°ƒæ•´ç»´åº¦
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),  
                nn.BatchNorm2d(out_channels)  # æ‰¹å½’ä¸€åŒ–å±‚
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))  # é€šè¿‡ç¬¬ä¸€å±‚å·ç§¯ã€æ‰¹å½’ä¸€åŒ–å’Œ ReLU æ¿€æ´»å‡½æ•°
        out = self.bn2(self.conv2(out))  # é€šè¿‡ç¬¬äºŒå±‚å·ç§¯å’Œæ‰¹å½’ä¸€åŒ–
        out += self.shortcut(x)  # å°†å¿«æ·è¿æ¥çš„ç»“æœä¸å·ç§¯ç»“æœç›¸åŠ 
        out = F.relu(out)  # é€šè¿‡ ReLU æ¿€æ´»å‡½æ•°
        return out  # è¿”å›æœ€ç»ˆçš„è¾“å‡º

class ResNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNet, self).__init__()
        self.in_channels = 64  # åˆå§‹è¾“å…¥é€šé“æ•°
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # ç¬¬ä¸€å±‚å·ç§¯
        self.bn1 = nn.BatchNorm2d(64)  # æ‰¹å½’ä¸€åŒ–å±‚
        self.layer1 = self._make_layer(64, 2, stride=1)  # ç¬¬ä¸€å±‚æ®‹å·®å—
        self.layer2 = self._make_layer(128, 2, stride=2)  # ç¬¬äºŒå±‚æ®‹å·®å—
        self.layer3 = self._make_layer(256, 2, stride=2)  # ç¬¬ä¸‰å±‚æ®‹å·®å—
        self.layer4 = self._make_layer(512, 2, stride=2)  # ç¬¬å››å±‚æ®‹å·®å—
        self.linear = nn.Linear(512, num_classes)  # å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºå¤§å°ä¸º num_classesï¼ˆ10 ä¸ªç±»åˆ«ï¼‰

    def _make_layer(self, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)  # æ¯ä¸ªæ®‹å·®å—çš„æ­¥é•¿
        layers = []  # å­˜å‚¨æ®‹å·®å—çš„åˆ—è¡¨
        for stride in strides:
            layers.append(ResidualBlock(self.in_channels, out_channels, stride))  # æ·»åŠ æ®‹å·®å—
            self.in_channels = out_channels  # æ›´æ–°è¾“å…¥é€šé“æ•°
        return nn.Sequential(*layers)  # è¿”å›ä¸€ä¸ªåŒ…å«å¤šä¸ªæ®‹å·®å—çš„åºåˆ—

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))  # é€šè¿‡ç¬¬ä¸€å±‚å·ç§¯ã€æ‰¹å½’ä¸€åŒ–å’Œ ReLU æ¿€æ´»å‡½æ•°
        out = self.layer1(out)  # é€šè¿‡ç¬¬ä¸€å±‚æ®‹å·®å—
        out = self.layer2(out)  # é€šè¿‡ç¬¬äºŒå±‚æ®‹å·®å—
        out = self.layer3(out)  # é€šè¿‡ç¬¬ä¸‰å±‚æ®‹å·®å—
        out = self.layer4(out)  # é€šè¿‡ç¬¬å››å±‚æ®‹å·®å—
        out = F.avg_pool2d(out, 4)  # é€šè¿‡å¹³å‡æ± åŒ–å±‚ä¸‹é‡‡æ ·
        out = out.view(out.size(0), -1)  # å°†ç‰¹å¾å±•å¹³ä¸ºä¸€ç»´å‘é‡
        out = self.linear(out)  # é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»
        return out  # è¿”å›æœ€ç»ˆçš„è¾“å‡º

# Vision Transformer (ViT) æ¨¡å‹
class PatchEmbedding(nn.Module):
    def __init__(self, img_size=28, patch_size=7, in_channels=1, embed_dim=64):
        super(PatchEmbedding, self).__init__()
        self.img_size = img_size  # è¾“å…¥å›¾åƒå¤§å°
        self.patch_size = patch_size  # æ¯ä¸ª patch çš„å¤§å°
        self.num_patches = (img_size // patch_size) ** 2  # patch çš„æ•°é‡
        # å·ç§¯å±‚ï¼Œå°†å›¾åƒåˆ†å‰²ä¸º patch å¹¶æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)  

    def forward(self, x):
        x = self.proj(x)  # é€šè¿‡å·ç§¯å±‚å°†å›¾åƒåˆ†å‰²ä¸º patch
        x = rearrange(x, 'b c h w -> b (h w) c')  # ä½¿ç”¨ rearrange å°† patch é‡æ–°æ’åˆ—ä¸ºåºåˆ—
        return x  # è¿”å›æœ€ç»ˆçš„è¾“å‡º

class TransformerEncoder(nn.Module):
    def __init__(self, embed_dim=64, num_heads=4, ff_dim=128, dropout=0.1):
        super(TransformerEncoder, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)  # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        self.norm1 = nn.LayerNorm(embed_dim)  # å±‚å½’ä¸€åŒ–
        self.norm2 = nn.LayerNorm(embed_dim)  # å±‚å½’ä¸€åŒ–
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),  # å‰é¦ˆç¥ç»ç½‘ç»œçš„ç¬¬ä¸€å±‚
            nn.GELU(),  # GELU æ¿€æ´»å‡½æ•°
            nn.Linear(ff_dim, embed_dim)  # å‰é¦ˆç¥ç»ç½‘ç»œçš„ç¬¬äºŒå±‚
        )
        self.dropout = nn.Dropout(dropout)  # éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

    def forward(self, x):
        attn_output, _ = self.attention(x, x, x)  # é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æå–ç‰¹å¾
        x = x + self.dropout(attn_output)  # é€šè¿‡æ®‹å·®è¿æ¥å’Œ dropout
        x = self.norm1(x)  # é€šè¿‡å±‚å½’ä¸€åŒ–
        ffn_output = self.ffn(x)  # é€šè¿‡å‰é¦ˆç¥ç»ç½‘ç»œ
        x = x + self.dropout(ffn_output)  # é€šè¿‡æ®‹å·®è¿æ¥å’Œ dropout
        x = self.norm2(x)  # é€šè¿‡å±‚å½’ä¸€åŒ–
        return x  # è¿”å›æœ€ç»ˆçš„è¾“å‡º

class ViT(nn.Module):
    def __init__(self, img_size=28, patch_size=7, in_channels=1, embed_dim=64, num_heads=4, ff_dim=128, num_layers=4, num_classes=10, dropout=0.1):
        super(ViT, self).__init__()
        # Patch Embedding å±‚
        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)  
        self.positional_embedding = nn.Parameter(torch.zeros(1, self.patch_embedding.num_patches + 1, embed_dim))  # ä½ç½®ç¼–ç 
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # åˆ†ç±» token
        self.transformer_encoder = nn.ModuleList([
            # å¤šä¸ª Transformer ç¼–ç å™¨å±‚
            TransformerEncoder(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)  
        ])
        self.norm = nn.LayerNorm(embed_dim)  # å±‚å½’ä¸€åŒ–
        self.fc = nn.Linear(embed_dim, num_classes)  # å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºå¤§å°ä¸º num_classesï¼ˆ10 ä¸ªç±»åˆ«ï¼‰

    def forward(self, x):
        x = self.patch_embedding(x)  # å°†å›¾åƒåˆ†å‰²ä¸º patch å¹¶æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # æ‰©å±•åˆ†ç±» token
        x = torch.cat((cls_token, x), dim=1)  # å°†åˆ†ç±» token å’Œ patch åºåˆ—æ‹¼æ¥
        x = x + self.positional_embedding  # æ·»åŠ ä½ç½®ç¼–ç 
        for layer in self.transformer_encoder:  # é€šè¿‡å¤šä¸ª Transformer ç¼–ç å™¨å±‚
            x = layer(x)
        x = self.norm(x)  # é€šè¿‡å±‚å½’ä¸€åŒ–
        cls_output = x[:, 0]  # æå–åˆ†ç±» token çš„è¾“å‡º
        out = self.fc(cls_output)  # é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»
        return out  # è¿”å›æœ€ç»ˆçš„è¾“å‡º

# è®­ç»ƒå‡½æ•°
def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):
    train_losses, val_losses = [], []  # å­˜å‚¨è®­ç»ƒå’ŒéªŒè¯æŸå¤±
    for epoch in range(epochs):
        model.train()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        running_loss = 0.0  # ç´¯è®¡æ¯ä¸ª epoch çš„æŸå¤±
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            outputs = model(inputs)  # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—è¾“å‡º
            loss = criterion(outputs, labels)  # è®¡ç®—æŸå¤±
            loss.backward()  # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
            optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°
            running_loss += loss.item()  # ç´¯è®¡æŸå¤±
        train_losses.append(running_loss / len(train_loader))  # è®°å½•æ¯ä¸ª epoch çš„è®­ç»ƒæŸå¤±

        model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        val_loss = 0.0  # ç´¯è®¡éªŒè¯é›†çš„æŸå¤±
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
                outputs = model(inputs)  # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—è¾“å‡º
                loss = criterion(outputs, labels)  # è®¡ç®—æŸå¤±
                val_loss += loss.item()  # ç´¯è®¡æŸå¤±
        val_losses.append(val_loss / len(val_loader))  # è®°å½•æ¯ä¸ª epoch çš„éªŒè¯æŸå¤±
        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}")  # æ‰“å°è®­ç»ƒå’ŒéªŒè¯æŸå¤±
    return train_losses, val_losses  # è¿”å›è®­ç»ƒå’ŒéªŒè¯æŸå¤±åˆ—è¡¨

# è®¡ç®—å‚æ•°é‡
def count_parameters(model):
    return sum(p.numel() for p in model.parameters())  # è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡

# è®¡ç®—æ¨¡å‹å¤§å°
def get_model_size(model):
    return sum(p.numel() * p.element_size() for p in model.parameters())  # è®¡ç®—æ¨¡å‹çš„å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰

# è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´ per sample
def measure_inference_time(model, test_loader):
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    total_time = 0.0  # ç´¯è®¡æ¨ç†æ—¶é—´
    num_samples = 0  # ç´¯è®¡æ ·æœ¬æ•°é‡
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
            start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
            outputs = model(inputs)  # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—è¾“å‡º
            end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
            total_time += end_time - start_time  # ç´¯è®¡æ¨ç†æ—¶é—´
            num_samples += inputs.size(0)  # ç´¯è®¡æ ·æœ¬æ•°é‡
    avg_time_per_sample = total_time / num_samples  # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡æ¨ç†æ—¶é—´
    return avg_time_per_sample  # è¿”å›å¹³å‡æ¨ç†æ—¶é—´

# è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
def evaluate_on_test_set(model, test_loader, criterion):
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    test_loss = 0.0  # ç´¯è®¡æµ‹è¯•é›†çš„æŸå¤±
    correct = 0  # ç´¯è®¡æ­£ç¡®é¢„æµ‹çš„æ•°é‡
    total = 0  # ç´¯è®¡æ ·æœ¬æ•°é‡
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
            outputs = model(inputs)  # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—è¾“å‡º
            loss = criterion(outputs, labels)  # è®¡ç®—æŸå¤±
            test_loss += loss.item()  # ç´¯è®¡æŸå¤±
            _, predicted = outputs.max(1)  # è·å–é¢„æµ‹ç»“æœ
            total += labels.size(0)  # ç´¯è®¡æ ·æœ¬æ•°é‡
            correct += predicted.eq(labels).sum().item()  # ç´¯è®¡æ­£ç¡®é¢„æµ‹çš„æ•°é‡
    test_loss /= len(test_loader)  # è®¡ç®—å¹³å‡æµ‹è¯•æŸå¤±
    test_accuracy = correct / total  # è®¡ç®—æµ‹è¯•å‡†ç¡®ç‡
    return test_loss, test_accuracy  # è¿”å›æµ‹è¯•æŸå¤±å’Œå‡†ç¡®ç‡

# å®šä¹‰ä¸åŒçš„å‚æ•°ç»„åˆ
learning_rates = [0.001, 0.01, 0.1]  # å­¦ä¹ ç‡åˆ—è¡¨
optimizers = [optim.Adam, optim.SGD]  # ä¼˜åŒ–å™¨åˆ—è¡¨
activation_functions = [F.relu, F.leaky_relu]  # æ¿€æ´»å‡½æ•°åˆ—è¡¨

# è®­ç»ƒå’Œè¯„ä¼°ä¸åŒå‚æ•°ç»„åˆ
results = {}  # å­˜å‚¨ä¸åŒå‚æ•°ç»„åˆçš„ç»“æœ
for lr in learning_rates:
    for opt in optimizers:
        for activation in activation_functions:
            model = CNN().to(device)  # åˆå§‹åŒ–æ¨¡å‹
            optimizer = opt(model.parameters(), lr=lr)  # åˆå§‹åŒ–ä¼˜åŒ–å™¨
            # æ‰“å°å½“å‰å‚æ•°ç»„åˆ
            print(f"Training with lr={lr}, optimizer={opt.__name__}, activation={activation.__name__}") 
            start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
            train_losses, val_losses = train(model, train_loader, val_loader, nn.CrossEntropyLoss(), optimizer, epochs=10)  # è®­ç»ƒæ¨¡å‹
            training_time = time.time() - start_time  # è®¡ç®—è®­ç»ƒæ—¶é—´
            # è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
            test_loss, test_accuracy = evaluate_on_test_set(model, test_loader, nn.CrossEntropyLoss()) 
            avg_inference_time = measure_inference_time(model, test_loader)  # è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´
            results[(lr, opt.__name__, activation.__name__)] = {  # å­˜å‚¨ç»“æœ
                'train_losses': train_losses,
                'val_losses': val_losses,
                'training_time': training_time,
                'test_loss': test_loss,
                'test_accuracy': test_accuracy,
                'parameters': count_parameters(model),
                'model_size': get_model_size(model),
                'avg_inference_time': avg_inference_time
            }

# ç»˜åˆ¶æ‰€æœ‰ Loss æ›²çº¿
def plot_all_loss_curves(results):
    fig, axes = plt.subplots(3, 1, figsize=(10, 15))  # åˆ›å»ºä¸€ä¸ªå¤§å›¾ï¼ŒåŒ…å« 3 è¡Œ 1 åˆ—çš„å­å›¾

    # ç»˜åˆ¶ä¸åŒå­¦ä¹ ç‡çš„ Loss æ›²çº¿
    for lr in learning_rates:
        key = (lr, 'Adam', 'relu')
        if key in results:
            train_losses = results[key]['train_losses']
            val_losses = results[key]['val_losses']
            axes[0].plot(train_losses, label=f'Train Loss (lr={lr})')  # ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
            axes[0].plot(val_losses, label=f'Val Loss (lr={lr})')  # ç»˜åˆ¶éªŒè¯æŸå¤±æ›²çº¿
    axes[0].set_title('Loss Curves for Different Learning Rates (Adam, ReLU)')  # è®¾ç½®æ ‡é¢˜
    axes[0].set_xlabel('Epoch')  # è®¾ç½® x è½´æ ‡ç­¾
    axes[0].set_ylabel('Loss')  # è®¾ç½® y è½´æ ‡ç­¾
    axes[0].legend()  # æ˜¾ç¤ºå›¾ä¾‹

    # ç»˜åˆ¶ä¸åŒä¼˜åŒ–å™¨çš„ Loss æ›²çº¿
    for opt in optimizers:
        key = (0.001, opt.__name__, 'relu')
        if key in results:
            train_losses = results[key]['train_losses']
            val_losses = results[key]['val_losses']
            axes[1].plot(train_losses, label=f'Train Loss (optimizer={opt.__name__})')  # ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
            axes[1].plot(val_losses, label=f'Val Loss (optimizer={opt.__name__})')  # ç»˜åˆ¶éªŒè¯æŸå¤±æ›²çº¿
    axes[1].set_title('Loss Curves for Different Optimizers (lr=0.001, ReLU)')  # è®¾ç½®æ ‡é¢˜
    axes[1].set_xlabel('Epoch')  # è®¾ç½® x è½´æ ‡ç­¾
    axes[1].set_ylabel('Loss')  # è®¾ç½® y è½´æ ‡ç­¾
    axes[1].legend()  # æ˜¾ç¤ºå›¾ä¾‹

    # ç»˜åˆ¶ä¸åŒæ¿€æ´»å‡½æ•°çš„ Loss æ›²çº¿
    for activation in activation_functions:
        key = (0.001, 'Adam', activation.__name__)
        if key in results:
            train_losses = results[key]['train_losses']
            val_losses = results[key]['val_losses']
            # ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
            axes[2].plot(train_losses, label=f'Train Loss (activation={activation.__name__})')  
            # ç»˜åˆ¶éªŒè¯æŸå¤±æ›²çº¿
            axes[2].plot(val_losses, label=f'Val Loss (activation={activation.__name__})')  
    axes[2].set_title('Loss Curves for Different Activation Functions (lr=0.001, Adam)')  # è®¾ç½®æ ‡é¢˜
    axes[2].set_xlabel('Epoch')  # è®¾ç½® x è½´æ ‡ç­¾
    axes[2].set_ylabel('Loss')  # è®¾ç½® y è½´æ ‡ç­¾
    axes[2].legend()  # æ˜¾ç¤ºå›¾ä¾‹

    plt.tight_layout()  # è°ƒæ•´å¸ƒå±€ï¼Œé¿å…é‡å 
    plt.show()  # æ˜¾ç¤ºç»˜åˆ¶çš„å›¾è¡¨

# è°ƒç”¨å‡½æ•°ç»˜åˆ¶æ‰€æœ‰ Loss æ›²çº¿
plot_all_loss_curves(results)

# è®¡ç®—å¤æ‚åº¦åˆ†æ
for params, result in results.items():
    print(f"Params: {params}")  # æ‰“å°å½“å‰å‚æ•°ç»„åˆ
    print(f"  Training Time: {result['training_time']:.2f} seconds")  # æ‰“å°è®­ç»ƒæ—¶é—´
    print(f"  Parameters: {result['parameters']}")  # æ‰“å°å‚æ•°é‡
    print(f"  Model Size: {result['model_size']} bytes")  # æ‰“å°æ¨¡å‹å¤§å°
 	# æ‰“å°å¹³å‡æ¨ç†æ—¶é—´
    print(f"  Average Inference Time per Sample: {result['avg_inference_time']:.6f} seconds") 
    # æ‰“å°æµ‹è¯•æŸå¤±å’Œå‡†ç¡®ç‡
    print(f"  Test Loss: {result['test_loss']:.4f}, Test Accuracy: {result['test_accuracy']:.4f}")  
    print()

# å®šä¹‰æ¨¡å‹åˆ—è¡¨
models = {
    'CNN': CNN().to(device),  # CNN æ¨¡å‹
    'ResNet': ResNet().to(device),  # ResNet æ¨¡å‹
    'ViT': ViT().to(device)  # ViT æ¨¡å‹
}

# è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
model_results = {}  # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„ç»“æœ
for name, model in models.items():
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    print(f"Training {name} model")  # æ‰“å°å½“å‰æ¨¡å‹åç§°
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    train_losses, val_losses = train(model, train_loader, val_loader, nn.CrossEntropyLoss(), optimizer, epochs=10)  # è®­ç»ƒæ¨¡å‹
    training_time = time.time() - start_time  # è®¡ç®—è®­ç»ƒæ—¶é—´
    test_loss, test_accuracy = evaluate_on_test_set(model, test_loader, nn.CrossEntropyLoss())  # è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
    avg_inference_time = measure_inference_time(model, test_loader)  # è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´
    model_results[name] = {  # å­˜å‚¨ç»“æœ
        'train_losses': train_losses,
        'val_losses': val_losses,
        'training_time': training_time,
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'parameters': count_parameters(model),
        'model_size': get_model_size(model),
        'avg_inference_time': avg_inference_time
    }

# ç»˜åˆ¶å¯¹æ¯”å›¾
for name, result in model_results.items():
    plt.plot(result['train_losses'], label=f'{name} Training Loss')  # ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
    plt.plot(result['val_losses'], label=f'{name} Validation Loss')  # ç»˜åˆ¶éªŒè¯æŸå¤±æ›²çº¿
plt.xlabel('Epoch')  # è®¾ç½® x è½´æ ‡ç­¾
plt.ylabel('Loss')  # è®¾ç½® y è½´æ ‡ç­¾
plt.legend()  # æ˜¾ç¤ºå›¾ä¾‹
plt.show()  # æ˜¾ç¤ºç»˜åˆ¶çš„å›¾è¡¨

# è®¡ç®—å¤æ‚åº¦åˆ†æ
for name, result in model_results.items():
    print(f"{name} - Parameters: {result['parameters']}, Model Size: {result['model_size']} bytes, Training Time: {result['training_time']:.2f} seconds, Average Inference Time per Sample: {result['avg_inference_time']:.6f} seconds, Test Loss: {result['test_loss']:.4f}, Test Accuracy: {result['test_accuracy']:.4f}")  # æ‰“å°å¤æ‚åº¦åˆ†æç»“æœ
```


